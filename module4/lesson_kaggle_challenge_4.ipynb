{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS7_lesson_kaggle_challenge_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZyiOteN16cD",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 2, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KMI2k-oBsS08"
      },
      "source": [
        "# Kaggle Challenge, Module 4\n",
        "\n",
        "- get and interpret the **confusion matrix** for classification models\n",
        "- use classification metrics: **precision, recall**\n",
        "- understand the relationships between precision, recall, **thresholds, and predicted probabilities**\n",
        "- understand how Precision@K can help **make decisions and allocate budgets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rU7RuVcjWdcp"
      },
      "source": [
        "### Setup\n",
        "\n",
        "You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpFoag9QoTgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "# If you're in Colab...\n",
        "if in_colab:\n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Install required python packages\n",
        "    !pip install -r requirements.txt\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa0jC4TY2Ksb",
        "colab_type": "text"
      },
      "source": [
        "#### Matplotlib version should _not_ == 3.1.1\n",
        "\n",
        "Because of this issue: [sns.heatmap top and bottom boxes are cut off](https://github.com/mwaskom/seaborn/issues/1773)\n",
        "\n",
        "> This was a matplotlib regression introduced in 3.1.1 which has been fixed in 3.1.2 (still forthcoming). For now the fix is to downgrade matplotlib to a prior version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZvKprTQ2Lsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "print(matplotlib.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZNCHldPn_iL",
        "colab_type": "text"
      },
      "source": [
        "### Review: Load data, fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Cjxzrwn_iL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import category_encoders as ce\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangles train, validate, and test sets in the same way\"\"\"\n",
        "    X = X.copy()\n",
        "\n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "    \n",
        "    # Extract components from date_recorded, then drop the original column\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    X['month_recorded'] = X['date_recorded'].dt.month\n",
        "    X['day_recorded'] = X['date_recorded'].dt.day\n",
        "    X = X.drop(columns='date_recorded')\n",
        "    \n",
        "    # Engineer feature: how many years from construction_year to date_recorded\n",
        "    X['years'] = X['year_recorded'] - X['construction_year']    \n",
        "    \n",
        "    # Drop recorded_by (never varies) and id (always varies, random)\n",
        "    unusable_variance = ['recorded_by', 'id']\n",
        "    X = X.drop(columns=unusable_variance)\n",
        "    \n",
        "    # Drop duplicate columns\n",
        "    duplicate_columns = ['quantity_group']\n",
        "    X = X.drop(columns=duplicate_columns)\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these like null values\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, np.nan)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values\n",
        "    cols_with_zeros = ['construction_year', 'longitude', 'latitude', 'gps_height', 'population']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "        \n",
        "    return X\n",
        "\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv('../data/tanzania/train_features.csv'), \n",
        "                 pd.read_csv('../data/tanzania/train_labels.csv'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv('../data/tanzania/test_features.csv')\n",
        "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')\n",
        "\n",
        "# Split train into train & val. Make val the same size as test.\n",
        "target = 'status_group'\n",
        "train, val = train_test_split(train, test_size=len(test),  \n",
        "                              stratify=train[target], random_state=42)\n",
        "\n",
        "# Wrangle train, validate, and test sets in the same way\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)\n",
        "\n",
        "# Arrange data into X features matrix and y target vector\n",
        "X_train = train.drop(columns=target)\n",
        "y_train = train[target]\n",
        "X_val = val.drop(columns=target)\n",
        "y_val = val[target]\n",
        "X_test = test\n",
        "\n",
        "# Make pipeline!\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(strategy='mean'), \n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_val)\n",
        "print('Validation Accuracy', accuracy_score(y_val, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfhziD2Wn_iO",
        "colab_type": "text"
      },
      "source": [
        "## Get and interpret the confusion matrix for classification models\n",
        "\n",
        "[Scikit-Learn User Guide — Confusion Matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MSWehj9n_iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP6FGBGUn_iQ",
        "colab_type": "text"
      },
      "source": [
        "#### How many correct predictions were made?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRSaYRPWn_iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q-3R7Ean_iT",
        "colab_type": "text"
      },
      "source": [
        "#### How many total predictions were made?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLAQL05fn_iT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1yQ_jYPn_iV",
        "colab_type": "text"
      },
      "source": [
        "#### What was the classification accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fskAC6SYn_iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqFgEm3tn_iY",
        "colab_type": "text"
      },
      "source": [
        "## Use classification metrics: precision, recall\n",
        "[Scikit-Learn User Guide — Classification Report](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGv7OLL4n_iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1U7HdC6n_ia",
        "colab_type": "text"
      },
      "source": [
        "#### Wikipedia, [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
        "\n",
        "> Both precision and recall are based on an understanding and measure of relevance.\n",
        "\n",
        "> Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12.\n",
        "\n",
        "> High precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50R-Xhwdn_ie",
        "colab_type": "text"
      },
      "source": [
        "#### [We can get precision & recall from the confusion matrix](https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIta6Vwsn_if",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY2rfzA4n_ih",
        "colab_type": "text"
      },
      "source": [
        "#### How many correct predictions of \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-anLkCin_ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYM6f99cn_ij",
        "colab_type": "text"
      },
      "source": [
        "#### How many total predictions of \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qCiA8j2n_ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXNuZ_Rnn_il",
        "colab_type": "text"
      },
      "source": [
        "#### What's the precision for \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1f7VsyXn_im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci4QguAkn_in",
        "colab_type": "text"
      },
      "source": [
        "#### How many actual \"non functional\" waterpumps?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlqxNhlYn_io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IY-vC-hn_iq",
        "colab_type": "text"
      },
      "source": [
        "#### What's the recall for \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U3v8lPP4KbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObVED_ugn_is",
        "colab_type": "text"
      },
      "source": [
        "## Understand the relationships between precision, recall, thresholds, and predicted probabilities. Understand how Precision@K can help make decisions and allocate budgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBcQQJ2kn_is",
        "colab_type": "text"
      },
      "source": [
        "### Imagine this scenario...\n",
        "\n",
        "Suppose there are over 14,000 waterpumps that you _do_ have some information about, but you _don't_ know whether they are currently functional, or functional but need repair, or non-functional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEEy86CHn_it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3az2llAAn_iu",
        "colab_type": "text"
      },
      "source": [
        "**You have the time and resources to go to just 2,000 waterpumps for proactive maintenance.** You want to predict, which 2,000 are most likely non-functional or in need of repair, to help you triage and prioritize your waterpump inspections.\n",
        "\n",
        "You have historical inspection data for over 59,000 other waterpumps, which you'll use to fit your predictive model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEWc2zt2n_iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train) + len(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2LiGJLin_ix",
        "colab_type": "text"
      },
      "source": [
        "You have historical inspection data for over 59,000 other waterpumps, which you'll use to fit your predictive model.\n",
        "\n",
        "Based on this historical data, if you randomly chose waterpumps to inspect, then about 46% of the waterpumps would need repairs, and 54% would not need repairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JliDXTp5n_iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLnJ7Fnan_i1",
        "colab_type": "text"
      },
      "source": [
        "**Can you do better than random at prioritizing inspections?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIh2Xj8fn_i3",
        "colab_type": "text"
      },
      "source": [
        "In this scenario, we should define our target differently. We want to identify which waterpumps are non-functional _or_ are functional but needs repair:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7naqusI0n_i4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train != 'functional'\n",
        "y_val = y_val != 'functional'\n",
        "y_train.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1UR1t8Zn_i6",
        "colab_type": "text"
      },
      "source": [
        "We already made our validation set the same size as our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHHIplB7n_i8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(val) == len(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g41DA70rn_i9",
        "colab_type": "text"
      },
      "source": [
        "We can refit our model, using the redefined target.\n",
        "\n",
        "Then make predictions for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXL0LaXQn_i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qISPzM43n_jA",
        "colab_type": "text"
      },
      "source": [
        "And look at the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y72fakpmn_jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M30BXR6Rn_jC",
        "colab_type": "text"
      },
      "source": [
        "#### How many total predictions of \"True\" (\"non functional\" or \"functional needs repair\") ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IeTJFo8n_jD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aZSdskSn_jF",
        "colab_type": "text"
      },
      "source": [
        "#### We don't have \"budget\" to take action on all these predictions\n",
        "\n",
        "- But we can get predicted probabilities, to rank the predictions. \n",
        "- Then change the threshold, to change the number of positive predictions, based on our budget."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXkfXDDZn_jF",
        "colab_type": "text"
      },
      "source": [
        "### Get predicted probabilities and plot the distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwfe7j7W_jTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD6pRFKOn_jH",
        "colab_type": "text"
      },
      "source": [
        "### Change the threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjOhH0BMB55A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "top80m_Gn_jI",
        "colab_type": "text"
      },
      "source": [
        "### In this scenario ... \n",
        "\n",
        "Accuracy _isn't_ the best metric!\n",
        "\n",
        "Instead, change the threshold, to change the number of positive predictions, based on the budget. (You have the time and resources to go to just 2,000 waterpumps for proactive maintenance.)\n",
        "\n",
        "Then, evaluate with the precision for \"non functional\"/\"functional needs repair\".\n",
        "\n",
        "This is conceptually like **Precision@K**, where k=2,000.\n",
        "\n",
        "Read more here: [Recall and Precision at k for Recommender Systems: Detailed Explanation with examples](https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54)\n",
        "\n",
        "> Precision at k is the proportion of recommended items in the top-k set that are relevant\n",
        "\n",
        "> Mathematically precision@k is defined as: `Precision@k = (# of recommended items @k that are relevant) / (# of recommended items @k)`\n",
        "\n",
        "> In the context of recommendation systems we are most likely interested in recommending top-N items to the user. So it makes more sense to compute precision and recall metrics in the first N items instead of all the items. Thus the notion of precision and recall at k where k is a user definable integer that is set by the user to match the top-N recommendations objective.\n",
        "\n",
        "We asked, can you do better than random at prioritizing inspections?\n",
        "\n",
        "If we had randomly chosen waterpumps to inspect, we estimate that only 920 waterpumps would be repaired after 2,000 maintenance visits. (46%)\n",
        "\n",
        "But using our predictive model, in the validation set, we succesfully identified over 1,600 waterpumps in need of repair!\n",
        "\n",
        "So we will use this predictive model with the dataset of over 14,000 waterpumps that we _do_ have some information about, but we _don't_ know whether they are currently functional, or functional but need repair, or non-functional.\n",
        "\n",
        "We will predict which 2,000 are most likely non-functional or in need of repair.\n",
        "\n",
        "We estimate that approximately 1,600 waterpumps will be repaired after these 2,000 maintenance visits.\n",
        "\n",
        "So we're confident that our predictive model will help triage and prioritize waterpump inspections."
      ]
    }
  ]
}