{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUhphDysw-6P"
      },
      "source": [
        "## BloomTech Data Science\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvWhcow-dvS8"
      },
      "source": [
        "# Cross-Validation\n",
        "\n",
        "- Do **k-fold cross-validation** with independent test set\n",
        "- Use scikit-learn for **hyperparameter optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRgkCSgGdty-"
      },
      "source": [
        "%%capture\n",
        "!pip install category_encoders==2.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF70Jp0x4NJU"
      },
      "source": [
        "from category_encoders import OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_val_score, validation_curve # k-fold CV\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # Hyperparameter tuning\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVzQtNC9oucF"
      },
      "source": [
        "# Downloading the Tanzania Waterpump Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-olXlnJoucR"
      },
      "source": [
        "Make sure  you only use the dataset that is available through the **DS** **Kaggle Competition**. DO NOT USE any other Tanzania waterpump datasets that you might find online.\n",
        "\n",
        "There are two ways you can get the dataset. Make sure you have joined the competition first!:\n",
        "\n",
        "1. You can download the dataset directly by accessing the challenge and the files through the Kaggle Competition URL on Canvas (make sure you have joined the competition!)\n",
        "\n",
        "2. Use the Kaggle API using the code in the following cells. This article provides helpful information on how to fetch your Kaggle Dataset into Google Colab using the Kaggle API.\n",
        "\n",
        "> https://medium.com/analytics-vidhya/how-to-fetch-kaggle-datasets-into-google-colab-ea682569851a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6TZ5nDFYkCa"
      },
      "source": [
        "# Using Kaggle API to download datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2e6fPUATxLZ",
        "outputId": "b8cb2a8b-707c-4da8-e5b6-ff9b88f48493"
      },
      "source": [
        "# mounting your google drive on colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYSpUv9uYBAo",
        "outputId": "2e6a88bf-6b0b-4a8e-aeac-e0919d4a78fb"
      },
      "source": [
        "#change your working directory, if you want to or have already saved your kaggle dataset on google drive.\n",
        "%cd /content/gdrive/My Drive/Kaggle\n",
        "# update it to your folder location on drive that contians the dataset and/or kaggle API token json file."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXChvgdZYb_t"
      },
      "source": [
        "# Download your Kaggle Dataset, if you haven't already done so.\n",
        "# import os\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "# !kaggle competitions download -c bloomtech-water-pump-challenge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB84qgRRYdDF"
      },
      "source": [
        "# Unzip your Kaggle dataset, if you haven't already done so.\n",
        "# !unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eag2zYiQYf6q"
      },
      "source": [
        "# List all files in your Kaggle folder on your google drive.\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCBYPw7kd1AN"
      },
      "source": [
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXMenXoZ0D6"
      },
      "source": [
        "def wrangle(fm_path, tv_path=None):\n",
        "  if tv_path:\n",
        "    df = pd.merge(pd.read_csv(fm_path,\n",
        "                              na_values=[0, -2.000000e-08],\n",
        "                              parse_dates=['date_recorded']),\n",
        "                  pd.read_csv(tv_path)).set_index('id')\n",
        "  else:\n",
        "    df = pd.read_csv(fm_path,\n",
        "                     na_values=[0, -2.000000e-08],\n",
        "                     parse_dates=['date_recorded'],\n",
        "                     index_col='id')\n",
        "\n",
        "  # Drop constant columns\n",
        "  df.drop(columns=['recorded_by'], inplace=True)\n",
        "\n",
        "  # Create age feature\n",
        "  df['pump_age'] = df['date_recorded'].dt.year - df['construction_year']\n",
        "  df.drop(columns='date_recorded', inplace=True)\n",
        "\n",
        "  # Drop HCCCs\n",
        "  cutoff = 100\n",
        "  drop_cols = [col for col in df.select_dtypes('object').columns\n",
        "              if df[col].nunique() > cutoff]\n",
        "  df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "  # Drop duplicate columns\n",
        "  dupe_cols = [col for col in df.head(100).T.duplicated().index # change 15 to 100!!!!\n",
        "               if df.head(100).T.duplicated()[col]]\n",
        "  df.drop(columns=dupe_cols, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "df = wrangle(fm_path='train_features.csv',\n",
        "             tv_path='train_labels.csv')\n",
        "\n",
        "X_test = wrangle(fm_path='test_features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wupyZysm_avz"
      },
      "source": [
        "df.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jetWccxMqmzY"
      },
      "source": [
        "# II. Split Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-MPA0qlr-mK"
      },
      "source": [
        "## Split TV from FM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'status_group'\n",
        "y = df[target]\n",
        "X = df.drop(columns = target)"
      ],
      "metadata": {
        "id": "-iN01DozUbyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8jSWomGsLsw"
      },
      "source": [
        "# Training-Validation Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX9uvMMgs6J_"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "This is a **classification** problem, our baseline will be **accuracy**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts(normalize = True).max()"
      ],
      "metadata": {
        "id": "PkUJVqSuULDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA19NsrFtgTT"
      },
      "source": [
        "# IV. Build Models\n",
        "\n",
        "- `DecisionTreeClassifier`\n",
        "- `RandomForestClassifier`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_ZpJSwObNl_"
      },
      "source": [
        "model_dt = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    DecisionTreeClassifier(random_state=42))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4fO6aXEbNiQ"
      },
      "source": [
        "model_rf = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    RandomForestClassifier(n_estimators =25, random_state=42)\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l96Atv4REyH3"
      },
      "source": [
        "**Check cross-validation scores**\n",
        "\n",
        "![Cross Validation](https://upload.wikimedia.org/wikipedia/commons/4/4b/KfoldCV.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLx-EHNGB-xq"
      },
      "source": [
        "print('CV score DecisionTreeClassifier')\n",
        "print('Mean CV accuracy score:', )\n",
        "print('STD CV accuracy score:',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLIdbPJpCC_S"
      },
      "source": [
        "print('CV score RandomForestClassifier')\n",
        "print('Mean CV accuracy score:',)\n",
        "print('STD CV accuracy score:',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSTF3aOWo-5-"
      },
      "source": [
        "# V. Tune Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcYCO2Cj4ILg"
      },
      "source": [
        "Different hyperparameter values I want to try out :\n",
        "\n",
        "*   Simpleimputer - mean, median - 2 values\n",
        "*   max_depth - range(5,40,5) - 7 values\n",
        "*   n_estimators - range(25,125,25) - 4 values\n",
        "\n",
        "> Total combinations of these hyperparameters = 2 * 7 * 4 = 56\n",
        "\n",
        "Testing out the above hyperparameter combinations with 5-fold Cross Validation will need :\n",
        "\n",
        "> Total number of models to be fit = 2 * 7 * 4 * 5 = 280\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgK-doeJWMAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrMYSoldWL7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRL-I2ensgom"
      },
      "source": [
        "**`GridSearch`:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f27tFQRMWARw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPvNOV6Cw1Zl"
      },
      "source": [
        "**`RandomizedSearchCV`:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFXI-drCV9LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wT5O4MpOV4qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6P_b_2u1V9II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdPydrAciFvW"
      },
      "source": [
        "# VI. Communicate Results\n",
        "\n",
        "**Showing Feature Importance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orrZCcWqj0Wu"
      },
      "source": [
        "Plot the feature importance for our `RandomForest` model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bestestimator =\n",
        "importances = bestestimator.named_steps['randomforestclassifier'].feature_importances_\n",
        "features = X.columns\n",
        "feat_imp = pd.Series(importances, index=features).sort_values()\n",
        "feat_imp.tail(10).plot(kind='barh')\n",
        "plt.xlabel('Reduction in Gini Impurity');"
      ],
      "metadata": {
        "id": "X3QZfcikWWW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIiBxuSrDPZU"
      },
      "source": [
        "# VII. Make Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbEIotkT8eUM"
      },
      "source": [
        "y_pred = model_rfrs.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFL1Zdi88f98"
      },
      "source": [
        "submission = pd.DataFrame({'status_group':y_pred}, index=X_test.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "cNjXGaXEM9Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvrFM81t9Fqv"
      },
      "source": [
        "pd.Timestamp.now().strftime('%Y-%m-%d_%H%M_')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDIlJ1fDPZW"
      },
      "source": [
        "datestamp = pd.Timestamp.now().strftime('%Y-%m-%d_%H%M_') #string from time format\n",
        "submission.to_csv(f'{datestamp}submission.csv') #format string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS6EDRbCbr-F"
      },
      "source": [
        "# VIII. Saving a trained model to reuse it in the future"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Once you have found the best model, you might as well save it and then reload it when you want to test it later\n",
        "\n",
        "# save model\n",
        "import pickle\n",
        "\n",
        "filename =\n",
        "\n",
        "#save your model (it will be stored in your current working directory - download to your computer if GDrive is not mounted)\n",
        "pickle.dump(model_rf,open(filename,'wb'))\n",
        "#load model\n",
        "model_rf_loaded = pickle.load(open(filename,'rb'))"
      ],
      "metadata": {
        "id": "T1H0NSTQUh-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}