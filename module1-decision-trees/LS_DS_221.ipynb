{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "LS_DS_221.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masonnystrom/DS-Unit-2-Kaggle-Challenge/blob/master/module1-decision-trees/LS_DS_221.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bs7Tr4Ubw35c"
      },
      "source": [
        "Mason Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 2, Module 1*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QX0jgn6FTo2I"
      },
      "source": [
        "# Decision Trees\n",
        "\n",
        "- clean data with **outliers and missing values**\n",
        "- use scikit-learn **pipelines**\n",
        "- use scikit-learn for **decision trees**\n",
        "- get and interpret **feature importances** of a tree-based model\n",
        "- understand why decision trees are useful to model **non-linear, non-monotonic** relationships and **feature interactions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u14XI47bFZTk"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Run the code cell below. You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab.\n",
        "\n",
        "Libraries\n",
        "\n",
        "- **category_encoders** \n",
        "- **graphviz**\n",
        "- ipywidgets\n",
        "- matplotlib\n",
        "- numpy\n",
        "- pandas\n",
        "- **plotly**\n",
        "- seaborn\n",
        "- scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OzeMxiloFV2y",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "    !pip install pandas-profiling==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UFGmW4ijn4YN"
      },
      "source": [
        "# Clean data with outliers and missing values ðŸš°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBJG0M1aCIN0",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8dU5w0JCIN1",
        "colab_type": "text"
      },
      "source": [
        "Real-world data is often dirty. [The Quartz guide to bad data](https://github.com/Quartz/bad-data-guide) is a \"reference to problems seen in real-world data along with suggestions on how to resolve them.\" One of the common issues is [\"Zeros replace missing values.\"](https://github.com/Quartz/bad-data-guide#zeros-replace-missing-values)\n",
        "\n",
        "This is true of the Tanzania Waterpumps data, which we'll use throughout your Kaggle Challenge sprint:\n",
        "\n",
        "> Using data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\n",
        "\n",
        "\n",
        "The Tanzania Waterpumps data has outliers and missing values. Let's start to clean it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yVpWy6fJyTYo"
      },
      "source": [
        "First, load & split data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dg8hVHaJTldG",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
        "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
        "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
        "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
        "\n",
        "# Split train into train & val\n",
        "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "train.shape, val.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1nuO7LICIN6",
        "colab_type": "text"
      },
      "source": [
        "This is a multi-class classification problem. The majority class occurs with 54% frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv-kirJMCIN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['status_group'].value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_VzmUwxCIN-",
        "colab_type": "text"
      },
      "source": [
        "One common issues is [\"zeros replace missing values.\"](https://github.com/Quartz/bad-data-guide#zeros-replace-missing-values)\n",
        "\n",
        "We can use [Pandas Profiling](https://github.com/pandas-profiling/pandas-profiling) to get a report, with warnings about frequent zeros, and much more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vP70cTZCIN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check Pandas Profiling version\n",
        "import pandas_profiling\n",
        "pandas_profiling.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at6Z1EqZCIOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Old code for Pandas Profiling version 2.3\n",
        "# It can be very slow with medium & large datasets.\n",
        "# These parameters will make it faster.\n",
        "\n",
        "# profile = train.profile_report(\n",
        "#     check_correlation_pearson=False,\n",
        "#     correlations={\n",
        "#         'pearson': False,\n",
        "#         'spearman': False,\n",
        "#         'kendall': False,\n",
        "#         'phi_k': False,\n",
        "#         'cramers': False,\n",
        "#         'recoded': False,\n",
        "#     },\n",
        "#     plot={'histogram': {'bayesian_blocks_bins': False}},\n",
        "# )\n",
        "#\n",
        "\n",
        "# New code for Pandas Profiling version 2.4\n",
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(train, minimal=True).to_notebook_iframe()\n",
        "\n",
        "profile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DeIQ1lDmqAHY"
      },
      "source": [
        "Note that longitude has 3% zeros. Some of the locations are at [\"Null Island\"](https://en.wikipedia.org/wiki/Null_Island) instead of Tanzania."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nOetSBQIHX3I",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "px.scatter(train, x='longitude', y='latitude', color='status_group', opacity=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FtWg_yIbyOsJ"
      },
      "source": [
        "The latitude is approximately zero, but not exactly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aP_SmUYzbUuP",
        "colab": {}
      },
      "source": [
        "train[['longitude', 'latitude']].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lBMWxVSCIOL",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V-OOtTMzqkhM"
      },
      "source": [
        "#### Define a function to wrangle train, validate, and test sets in the same way.\n",
        "\n",
        "Fix the location, and do more data cleaning and feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QwOx78FgHxHp",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "    \n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    cols_with_zeros = ['longitude', 'latitude']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "            \n",
        "    # quantity & quantity_group are duplicates, so drop one\n",
        "    X = X.drop(columns='quantity_group')\n",
        "    \n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sMqlStuWqwde"
      },
      "source": [
        "Now the locations look better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2OEYMZvdY0-X",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "px.scatter(train, x='longitude', y='latitude', color='status_group', opacity=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zSOF7clsY23F"
      },
      "source": [
        "By the way, you can also make maps wiith Plotly Express. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dbdljEmCVpf0",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# https://plot.ly/python/mapbox-layers/#base-maps-in-layoutmapboxstyle\n",
        "fig = px.scatter_mapbox(train, lat='latitude', lon='longitude', color='status_group', opacity=0.1)\n",
        "fig.update_layout(mapbox_style='stamen-terrain')\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ypjt052Lrmgn"
      },
      "source": [
        "#### Select features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "666FUbXOXgL8",
        "colab": {}
      },
      "source": [
        "# The status_group column is the target\n",
        "target = 'status_group'\n",
        "\n",
        "# Get a dataframe with all train columns except the target & id\n",
        "train_features = train.drop(columns=[target, 'id'])\n",
        "\n",
        "# Get a list of the numeric features\n",
        "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# Get a series with the cardinality of the nonnumeric features\n",
        "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
        "\n",
        "# Get a list of all categorical features with cardinality <= 50\n",
        "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
        "\n",
        "# Combine the lists \n",
        "features = numeric_features + categorical_features\n",
        "print(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4nWGAs5BWb1t",
        "colab": {}
      },
      "source": [
        "# Arrange data into X features matrix and y target vector \n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "X_test = test[features]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHb42XOmCIOa",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "For your Kaggle Challenge, define a function to wrangle train, validate, and test sets in the same way. Clean outliers and engineer features. \n",
        "\n",
        "(For example, what other columns have zeros and shouldn't? What other columns are duplicates, or nearly duplicates? Can you extract the year from date_recorded? Can you engineer new features, such as the number of years from waterpump construction to waterpump inspection?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "moC9WikFrqJV"
      },
      "source": [
        "# Use scikit-learn pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Gbjh5wCIOb",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aztpr-2hCIOc",
        "colab_type": "text"
      },
      "source": [
        "We can combine steps with pipelines: Encode, Impute, Scale, Fit, Predict!\n",
        "\n",
        "[The Scikit-Learn User Guide explains why pipelines are useful](https://scikit-learn.org/stable/modules/compose.html), and demonstrates how to use them.\n",
        "\n",
        "> Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
        "> - **Convenience and encapsulation.** You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
        "> - **Joint parameter selection.** You can grid search over parameters of all estimators in the pipeline at once.\n",
        "> - **Safety.** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PjRahqVMbDjz"
      },
      "source": [
        "#### Just for comparison's sake, here's preprocessing + Logistic Regression _without_ using a pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eyy2iYhCdRah"
      },
      "source": [
        "Here's the documentation for each step in the process:\n",
        "\n",
        "- https://contrib.scikit-learn.org/categorical-encoding/onehot.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zcT3gcE0aKai",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "imputer = SimpleImputer()\n",
        "scaler = StandardScaler()\n",
        "model = LogisticRegression(multi_class='auto', solver='lbfgs', n_jobs=-1)\n",
        "\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_train_imputed = imputer.fit_transform(X_train_encoded)\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "X_val_encoded = encoder.transform(X_val)\n",
        "X_val_imputed = imputer.transform(X_val_encoded)\n",
        "X_val_scaled = scaler.transform(X_val_imputed)\n",
        "print('Validation Accuracy', model.score(X_val_scaled, y_val))\n",
        "\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "X_test_imputed = imputer.transform(X_test_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "y_pred = model.predict(X_test_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnPtfEtqCIOh",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X68B66nAbIHS"
      },
      "source": [
        "#### Pipelines can help you write more concise code with fewer errors!\n",
        "\n",
        "Let's rewrite the code cell above, but with a pipeline.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ntflqYdszSa-",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7EQOYZtqr00F"
      },
      "source": [
        "#### Get and plot coefficients\n",
        "\n",
        "This is slightly harder when using pipelines.\n",
        "\n",
        "The pipeline doesn't have a `.coef_` attribute. But the model inside the pipeline does. \n",
        "\n",
        "So, here's [how to access steps inside a pipeline](https://scikit-learn.org/stable/modules/compose.html#accessing-steps):\n",
        "\n",
        "> Pipelineâ€™s `named_steps` attribute allows accessing steps by name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RFlfjFN1gyjI",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = pipeline.named_steps['logisticregression']\n",
        "encoder = pipeline.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val).columns\n",
        "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
        "plt.figure(figsize=(10,30))\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II9Ahgx0CIOn",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "For your assignment, try to use a scikit-learn pipeline. Do you consider it more convenient?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3_J_Imz96wPl"
      },
      "source": [
        "# Use scikit-learn for decision trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkyTF_jUCIOo",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this Unit, we learn about two \"families\" or types of models: Linear Models, and Trees.\n",
        "\n",
        "We'll start today with using [Decision Trees](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/), then understanding how they work. (See the Sources heading at the bottom of the notebook for lots of great links!)\n",
        "\n",
        "Later, we'll learn about Random Forests, and Gradient Boosted Trees, which are \"ensemble\" models made of many trees. In practice, these \"Tree Ensembles\" are used more often than a single Decision Tree. \n",
        "\n",
        "But we start with a single Tree because it's a helpful prerequisite for understanding Tree Ensembles. Also, a single, shallow decision tree is a useful baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi3d533-CIOo",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "86L3lownYE-e"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "\n",
        "We will start with default parameters, including:\n",
        "- `max_depth=None`\n",
        "- `min_samples_leaf=1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-1_grdR-AVyJ",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e5tupFXLjj6B"
      },
      "source": [
        "### Plot the decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jA1SmBwi6H1R",
        "colab": {}
      },
      "source": [
        "# Plot tree\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "model = pipeline.named_steps['decisiontreeclassifier']\n",
        "encoder = pipeline.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val).columns\n",
        "\n",
        "dot_data = export_graphviz(model, \n",
        "                           out_file=None, \n",
        "                           max_depth=3, \n",
        "                           feature_names=encoded_columns,\n",
        "                           class_names=model.classes_, \n",
        "                           impurity=False, \n",
        "                           filled=True, \n",
        "                           proportion=True, \n",
        "                           rounded=True)   \n",
        "display(graphviz.Source(dot_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nZYdLLQoMcXp"
      },
      "source": [
        "### Reduce complexity of the decision tree\n",
        "\n",
        "We can use the `min_samples_leaf` parameter to reduce model complexity.\n",
        "\n",
        "It's explained in [scikit-learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
        "\n",
        "Also in [A Visual Introduction to Machine Learning, Part 2](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/):\n",
        "\n",
        "> Models can be adjusted to change the way they fit the data. These 'settings' are called [hyper]parameters. An example of a decision-tree [hyper]parameter is the _minimum node size,_ which regulates the creation of new splits. A node will not split if the number of data points it contains is below the minimum node size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uGv9jYsmgSr6",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZw_HrL0CIOw",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Fit a Decision Tree classifier to make new submissions for your Kaggle challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cUW3dabWAWPk"
      },
      "source": [
        "# Get and interpret feature importances of a tree-based model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYadRkJZCIOy",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Linear models have coefficients, but trees don't. Instead, they have \"feature importances.\"\n",
        "\n",
        "Feature importances are always positive numbers, they don't have a sign. Feature importance measures how early & often a feature is used for the tree's \"branching\" decisions. \n",
        "\n",
        "Like most predictive modeling tools and techniques, feature importances are useful, but have tradeoffs, make assumptions, and can be misinterpreted. We'll continue to discuss through this lesson and the unit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGL8gXqJCIOy",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2dSAxgO9sLsx"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YVJcNKY_sso-",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csRRZi3sCIO2",
        "colab_type": "text"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "For your assignment, get and plot your feature importances. What features are most important in your model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KkV4TjcNAPzp"
      },
      "source": [
        "# Understand why decision trees are useful to model non-linear, non-monotonic relationships and feature interactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zspLRSx2CIO3",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wwmS5P3CIO4",
        "colab_type": "text"
      },
      "source": [
        "#### What does _(non)monotonic_ mean?!?!\n",
        "- See Figures 1-3 in Wikipedia's article, [Monotonic function](https://en.wikipedia.org/wiki/Monotonic_function)\n",
        "- See [World Population Growth, 1700-2010](https://ourworldindata.org/world-population-growth-past-future). World Population is non-linear and monotonic. Annual growth rate is non-linear and non-monotonic.\n",
        "- See [Accidents per Mile Driven, by Driver Age](http://howwedrive.com/2009/02/20/whats-the-real-risk-of-older-drivers/). This is non-linear and non-monotonic.\n",
        "\n",
        "#### What does _feature interactions_ mean?!?!\n",
        "- See the explanation in [_Interpretable Machine Learning,_ Chapter 5.4.1, Feature Interaction](https://christophm.github.io/interpretable-ml-book/interaction.html#feature-interaction).\n",
        "- See the exploration in this notebook, under the heading ***Example #3: Simple housing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJJGh3IZCIO5",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMW9JbD6wZ28"
      },
      "source": [
        "### Example 1: Predict which waterpumps are functional, just based on location\n",
        "(2 features, non-linear, feature interactions, classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0s7t37euwd41"
      },
      "source": [
        "### Compare a Logistic Regression with 2 features, longitude & latitude ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44TUfQpeo64d",
        "colab": {}
      },
      "source": [
        "train_location = X_train[['longitude', 'latitude']].copy()\n",
        "val_location = X_val[['longitude', 'latitude']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X_xIa_1Kozfl",
        "colab": {}
      },
      "source": [
        "# With just long & lat, a Logistic Regression can't beat the majority classifier baseline\n",
        "\n",
        "lr = make_pipeline(\n",
        "    SimpleImputer(),\n",
        "    LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
        ")\n",
        "\n",
        "lr.fit(train_location, y_train)\n",
        "print('Logistic Regression:')\n",
        "print('Train Accuracy', lr.score(train_location, y_train))\n",
        "print('Validation Accuracy', lr.score(val_location, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yD31msBkwlVQ"
      },
      "source": [
        "### ... versus a Decision Tree Classifier with 2 features, longitude & latitude\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2RbY4s-vjiMw",
        "colab": {}
      },
      "source": [
        "# With the same 2 features, the Decision Tree's validation accuracy is \n",
        "# almost 10 percentage points better\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = make_pipeline(\n",
        "    SimpleImputer(), \n",
        "    DecisionTreeClassifier(max_depth=16, random_state=42)\n",
        ")\n",
        "\n",
        "dt.fit(train_location, y_train)\n",
        "print('Decision Tree:')\n",
        "print('Train Accuracy', dt.score(train_location, y_train))\n",
        "print('Validation Accuracy', dt.score(val_location, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jFvsu2onNXw7"
      },
      "source": [
        "### Visualize the logistic regression predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48O3emQgCIPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import itertools\n",
        "from math import floor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def pred_heatmap(model, X, features, class_index=-1, title='', num=100):\n",
        "    \"\"\"\n",
        "    Visualize predicted probabilities, for classifier fit on 2 numeric features\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : scikit-learn classifier, already fit\n",
        "    X : pandas dataframe, which was used to fit model\n",
        "    features : list of strings, column names of the 2 numeric features\n",
        "    class_index : integer, index of class label\n",
        "    title : string, title of plot\n",
        "    num : int, number of grid points for each feature\n",
        "    \"\"\"\n",
        "    feature1, feature2 = features\n",
        "    min1, max1 = X[feature1].min(), X[feature1].max()\n",
        "    min2, max2 = X[feature2].min(), X[feature2].max()\n",
        "    x1 = np.linspace(min1, max1, num)\n",
        "    x2 = np.linspace(max2, min2, num)\n",
        "    combos = list(itertools.product(x1, x2))\n",
        "    y_pred_proba = model.predict_proba(combos)[:, class_index]\n",
        "    pred_grid = y_pred_proba.reshape(num, num).T\n",
        "    table = pd.DataFrame(pred_grid, columns=x1, index=x2)\n",
        "    plot_every_n_ticks = int(floor(num/4))\n",
        "    sns.heatmap(table, # vmin=0, vmax=1, \n",
        "                xticklabels=plot_every_n_ticks, \n",
        "                yticklabels=plot_every_n_ticks)\n",
        "    plt.xlabel(feature1)\n",
        "    plt.ylabel(feature2)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3yeINd4f8bRF",
        "colab": {}
      },
      "source": [
        "pred_heatmap(lr, train_location, features=['longitude', 'latitude'], \n",
        "             class_index=0, title='Logstic Regression, predicted probability, \"functional\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ebepdSHp4X4a",
        "colab": {}
      },
      "source": [
        "pd.Series(lr.named_steps['logisticregression'].coef_[0], \n",
        "          train_location.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lJZSMvW7Nd97"
      },
      "source": [
        "### Visualize the decision tree predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mJ-QtMDK-uX2",
        "colab": {}
      },
      "source": [
        "pred_heatmap(dt, train_location, features=['longitude', 'latitude'], \n",
        "             class_index=0, title='Decision Tree, predicted probability, \"functional\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wdjg7U_jNlJG"
      },
      "source": [
        "### How does a tree grow? Branch by branch!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_Jhv0_VId9B",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "for max_depth in [1,2,3]:\n",
        "    \n",
        "    # Fit decision tree\n",
        "    dt = make_pipeline(\n",
        "        SimpleImputer(), \n",
        "        DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
        "    )\n",
        "    dt.fit(train_location, y_train)\n",
        "    \n",
        "    # Display depth & scores\n",
        "    display(HTML(f'Max Depth {max_depth}'))\n",
        "    display(HTML(f'Train Accuracy {dt.score(train_location, y_train):.2f}'))\n",
        "    display(HTML(f'Validation Accuracy {dt.score(val_location, y_val):.2f}'))\n",
        "    \n",
        "    # Plot heatmap of predicted probabilities\n",
        "    pred_heatmap(dt, train_location, features=['longitude', 'latitude'], \n",
        "                 class_index=0, title='Predicted probability, \"functional\"')\n",
        "    \n",
        "    # Plot tree\n",
        "    # https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
        "    dot_data = export_graphviz(dt.named_steps['decisiontreeclassifier'], \n",
        "                               out_file=None, \n",
        "                               max_depth=3, \n",
        "                               feature_names=train_location.columns,\n",
        "                               class_names=dt.classes_, \n",
        "                               impurity=False, \n",
        "                               filled=True, \n",
        "                               proportion=True, \n",
        "                               rounded=True)   \n",
        "    display(graphviz.Source(dot_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qm9ta54J66CV",
        "colab": {}
      },
      "source": [
        "for max_depth in range(4,21):\n",
        "    \n",
        "    # Fit decision tree\n",
        "    dt = make_pipeline(\n",
        "        SimpleImputer(), \n",
        "        DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
        "    )\n",
        "    dt.fit(train_location, y_train)\n",
        "    \n",
        "    # Display depth & scores\n",
        "    display(HTML(f'Max Depth {max_depth}'))\n",
        "    display(HTML(f'Train Accuracy {dt.score(train_location, y_train):.2f}'))\n",
        "    display(HTML(f'Validation Accuracy {dt.score(val_location, y_val):.2f}'))\n",
        "    \n",
        "    # Plot heatmap of predicted probabilities\n",
        "    pred_heatmap(dt, train_location, features=['longitude', 'latitude'], \n",
        "                 class_index=0, title='Predicted probability, \"functional\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5zlKegIZBbGr"
      },
      "source": [
        "## Example 2: predicting golf putts\n",
        "(1 feature, non-linear, regression)\n",
        "\n",
        "https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GjvPHk-FBayo",
        "colab": {}
      },
      "source": [
        "columns = ['distance', 'tries', 'successes']\n",
        "data = [[2, 1443, 1346],\n",
        "        [3, 694, 577],\n",
        "        [4, 455, 337],\n",
        "        [5, 353, 208],\n",
        "        [6, 272, 149],\n",
        "        [7, 256, 136],\n",
        "        [8, 240, 111],\n",
        "        [9, 217, 69],\n",
        "        [10, 200, 67],\n",
        "        [11, 237, 75],\n",
        "        [12, 202, 52],\n",
        "        [13, 192, 46],\n",
        "        [14, 174, 54],\n",
        "        [15, 167, 28],\n",
        "        [16, 201, 27],\n",
        "        [17, 195, 31],\n",
        "        [18, 191, 33],\n",
        "        [19, 147, 20],\n",
        "        [20, 152, 24]]\n",
        "\n",
        "putts = pd.DataFrame(columns=columns, data=data)\n",
        "putts['rate of success'] = putts['successes'] / putts['tries']\n",
        "putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tr-83jOEBsxK"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x988kCamBpbn",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "putts_X = putts[['distance']]\n",
        "putts_y = putts['rate of success']\n",
        "lr = LinearRegression()\n",
        "lr.fit(putts_X, putts_y)\n",
        "print('R^2 Score', lr.score(putts_X, putts_y))\n",
        "ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
        "ax.plot(putts_X, lr.predict(putts_X));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MjLGrZjfFGD-"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EpFIetrsB2yc",
        "colab": {}
      },
      "source": [
        "import graphviz\n",
        "from ipywidgets import interact\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
        "\n",
        "def viztree(decision_tree, feature_names):\n",
        "    dot_data = export_graphviz(decision_tree, out_file=None, feature_names=feature_names, \n",
        "                               filled=True, rounded=True)   \n",
        "    return graphviz.Source(dot_data)\n",
        "\n",
        "def putts_tree(max_depth=1):\n",
        "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
        "    tree.fit(putts_X, putts_y)\n",
        "    print('R^2 Score', tree.score(putts_X, putts_y))\n",
        "    ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
        "    ax.step(putts_X, tree.predict(putts_X), where='mid')\n",
        "    plt.show()\n",
        "    display(viztree(tree, feature_names=['distance']))\n",
        "\n",
        "interact(putts_tree, max_depth=(1,6,1));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WBQJB0PVFtIE"
      },
      "source": [
        "## Example 3a: Simple housing \n",
        "(2 features, regression)\n",
        "\n",
        "https://christophm.github.io/interpretable-ml-book/interaction.html#feature-interaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SHQ6oh1uFzHO",
        "colab": {}
      },
      "source": [
        "columns = ['Price', 'Good Location', 'Big Size']\n",
        "\n",
        "data = [[300000, 1, 1], \n",
        "        [200000, 1, 0], \n",
        "        [250000, 0, 1], \n",
        "        [150000, 0, 0]]\n",
        "\n",
        "house = pd.DataFrame(columns=columns, data=data)\n",
        "house"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cH8IVe6GGAsm"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9uMiKdzF6Ud",
        "colab": {}
      },
      "source": [
        "house_X = house.drop(columns='Price')\n",
        "house_y = house['Price']\n",
        "lr = LinearRegression()\n",
        "lr.fit(house_X, house_y)\n",
        "print('R^2', lr.score(house_X, house_y))\n",
        "print('Intercept \\t', lr.intercept_)\n",
        "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
        "print(coefficients.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q7AASlWVGMD4"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sWlqO_usGKS8",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "tree.fit(house_X, house_y)\n",
        "print('R^2', tree.score(house_X, house_y))\n",
        "viztree(tree, feature_names=house_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MZkaOREUGzYp"
      },
      "source": [
        "## Example 3b: Simple housing, with a twist: _Feature Interaction_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6HE0_pibGzJj",
        "colab": {}
      },
      "source": [
        "house.loc[0, 'Price'] = 400000\n",
        "house_X = house.drop(columns='Price')\n",
        "house_y = house['Price']\n",
        "house"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rh7ZJe7GHCSp"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YYINxJkdG_Q2",
        "colab": {}
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(house_X, house_y)\n",
        "print('R^2', lr.score(house_X, house_y))\n",
        "print('Intercept \\t', lr.intercept_)\n",
        "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
        "print(coefficients.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9KIVsFO_HSmS"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r6JYZBZBHIX2",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "tree.fit(house_X, house_y)\n",
        "print('R^2', tree.score(house_X, house_y))\n",
        "viztree(tree, feature_names=house_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GMESefRCIPv",
        "colab_type": "text"
      },
      "source": [
        "# Review\n",
        "\n",
        "Decision Trees are useful and powerful for predictive modeling. Try using trees as you continue to participate in the Kaggle challenge!\n",
        "\n",
        "- Do train/validate/test split with the Tanzania Waterpumps data.\n",
        "- Define a function to wrangle train, validate, and test sets in the same way. Clean outliers and engineer features. \n",
        "- Select features. Use a scikit-learn pipeline to encode categoricals, impute missing values, and fit a decision tree classifier.\n",
        "- Get your validation accuracy score.\n",
        "- Get and plot your feature importances.\n",
        "- Submit your predictions to our Kaggle competition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LS81_UdCIPv",
        "colab_type": "text"
      },
      "source": [
        "# Sources\n",
        "\n",
        "- A Visual Introduction to Machine Learning\n",
        "  - [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
        "  - [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
        "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
        "- [How a Russian mathematician constructed a decision tree â€” by hand â€” to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
        "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
        "- [Letâ€™s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU) â€” _Donâ€™t worry about understanding the code, just get introduced to the concepts. This 10 minute video has excellent diagrams and explanations._\n",
        "- [Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)"
      ]
    }
  ]
}