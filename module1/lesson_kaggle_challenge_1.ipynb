{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS7_lesson_kaggle_challenge_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs7Tr4Ubw35c",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 2, Module 1*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QX0jgn6FTo2I"
      },
      "source": [
        "# Kaggle Challenge, Module 1\n",
        "\n",
        "\n",
        "#### Objectives\n",
        "- clean outliers, **impute** missing values\n",
        "- use scikit-learn **pipelines**\n",
        "- use scikit-learn for **decision trees**\n",
        "- get and interpret **feature importances** of a tree-based model\n",
        "- understand why decision trees are useful to model **non-linear, non-monotonic** relationships and **feature interactions**\n",
        "\n",
        "\n",
        "#### Links\n",
        "\n",
        "- A Visual Introduction to Machine Learning\n",
        "  - [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
        "  - [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
        "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
        "- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
        "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
        "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU) — _Don’t worry about understanding the code, just get introduced to the concepts. This 10 minute video has excellent diagrams and explanations._\n",
        "- [Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u14XI47bFZTk",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "\n",
        "You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzeMxiloFV2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "# If you're in Colab...\n",
        "if in_colab:\n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Install required python packages\n",
        "    !pip install -r requirements.txt\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2aRilw4DPi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this function later\n",
        "\n",
        "%matplotlib inline\n",
        "import itertools\n",
        "from math import floor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def pred_heatmap(model, X, features, class_index=-1, title='', num=100):\n",
        "    \"\"\"\n",
        "    Visualize predicted probabilities, for classifier fit on 2 numeric features\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : scikit-learn classifier, already fit\n",
        "    X : pandas dataframe, which was used to fit model\n",
        "    features : list of strings, column names of the 2 numeric features\n",
        "    class_index : integer, index of class label\n",
        "    title : string, title of plot\n",
        "    num : int, number of grid points for each feature\n",
        "    \"\"\"\n",
        "    feature1, feature2 = features\n",
        "    min1, max1 = X[feature1].min(), X[feature1].max()\n",
        "    min2, max2 = X[feature2].min(), X[feature2].max()\n",
        "    x1 = np.linspace(min1, max1, num)\n",
        "    x2 = np.linspace(max2, min2, num)\n",
        "    combos = list(itertools.product(x1, x2))\n",
        "    y_pred_proba = model.predict_proba(combos)[:, class_index]\n",
        "    pred_grid = y_pred_proba.reshape(num, num).T\n",
        "    table = pd.DataFrame(pred_grid, columns=x1, index=x2)\n",
        "    plot_every_n_ticks = int(floor(num/4))\n",
        "    sns.heatmap(table, xticklabels=plot_every_n_ticks, yticklabels=plot_every_n_ticks)\n",
        "    plt.xlabel(feature1)\n",
        "    plt.ylabel(feature2)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UFGmW4ijn4YN"
      },
      "source": [
        "# Clean outliers, impute missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVpWy6fJyTYo",
        "colab_type": "text"
      },
      "source": [
        "First, load & split data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dg8hVHaJTldG",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train = pd.merge(pd.read_csv('../data/tanzania/train_features.csv'), \n",
        "                 pd.read_csv('../data/tanzania/train_labels.csv'))\n",
        "test = pd.read_csv('../data/tanzania/test_features.csv')\n",
        "sample_submission = pd.read_csv('../data/tanzania/sample_submission.csv')\n",
        "\n",
        "# Split train into train & val\n",
        "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "train.shape, val.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DeIQ1lDmqAHY"
      },
      "source": [
        "Some of the locations are at [\"Null Island\"](https://en.wikipedia.org/wiki/Null_Island) instead of Tanzania."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOetSBQIHX3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "px.scatter(train, x='longitude', y='latitude', color='status_group', opacity=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtWg_yIbyOsJ",
        "colab_type": "text"
      },
      "source": [
        "The latitude is approximately zero, but not exactly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP_SmUYzbUuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[['longitude', 'latitude']].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V-OOtTMzqkhM"
      },
      "source": [
        "#### Define a function to wrangle train, validate, and test sets in the same way.\n",
        "\n",
        "Fix the location, and do more data cleaning and feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwOx78FgHxHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "    \n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    cols_with_zeros = ['longitude', 'latitude']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "            \n",
        "    # quantity & quantity_group are duplicates, so drop one\n",
        "    X = X.drop(columns='quantity_group')\n",
        "    \n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sMqlStuWqwde"
      },
      "source": [
        "Now the locations look better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OEYMZvdY0-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "px.scatter(train, x='longitude', y='latitude', color='status_group', opacity=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSOF7clsY23F",
        "colab_type": "text"
      },
      "source": [
        "By the way, you can also make maps wiith Plotly Express. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbdljEmCVpf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://plot.ly/python/mapbox-layers/#base-maps-in-layoutmapboxstyle\n",
        "fig = px.scatter_mapbox(train, lat='latitude', lon='longitude', color='status_group', opacity=0.1)\n",
        "fig.update_layout(mapbox_style='stamen-terrain')\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ypjt052Lrmgn"
      },
      "source": [
        "#### Select features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "666FUbXOXgL8",
        "colab": {}
      },
      "source": [
        "# The status_group column is the target\n",
        "target = 'status_group'\n",
        "\n",
        "# Get a dataframe with all train columns except the target & id\n",
        "train_features = train.drop(columns=[target, 'id'])\n",
        "\n",
        "# Get a list of the numeric features\n",
        "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# Get a series with the cardinality of the nonnumeric features\n",
        "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
        "\n",
        "# Get a list of all categorical features with cardinality <= 50\n",
        "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
        "\n",
        "# Combine the lists \n",
        "features = numeric_features + categorical_features\n",
        "print(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nWGAs5BWb1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arrange data into X features matrix and y target vector \n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "X_test = test[features]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "moC9WikFrqJV"
      },
      "source": [
        "# Use scikit-learn pipelines\n",
        "\n",
        "We can combine steps with pipelines: Encode, Impute, Scale, Fit, Predict!\n",
        "\n",
        "[The Scikit-Learn User Guide explains why pipelines are useful](https://scikit-learn.org/stable/modules/compose.html), and demonstrates how to use them.\n",
        "\n",
        "> Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
        "> - **Convenience and encapsulation.** You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
        "> - **Joint parameter selection.** You can grid search over parameters of all estimators in the pipeline at once.\n",
        "> - **Safety.** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjRahqVMbDjz",
        "colab_type": "text"
      },
      "source": [
        "#### For comparison, here's preprocessing + Logistic Regression _without_ using a pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyy2iYhCdRah",
        "colab_type": "text"
      },
      "source": [
        "Here's the documentation for each step in the process:\n",
        "\n",
        "- https://contrib.scikit-learn.org/categorical-encoding/onehot.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcT3gcE0aKai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "imputer = SimpleImputer()\n",
        "scaler = StandardScaler()\n",
        "model = LogisticRegression(multi_class='auto', solver='lbfgs', n_jobs=-1)\n",
        "\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_train_imputed = imputer.fit_transform(X_train_encoded)\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "X_val_encoded = encoder.transform(X_val)\n",
        "X_val_imputed = imputer.transform(X_val_encoded)\n",
        "X_val_scaled = scaler.transform(X_val_imputed)\n",
        "print('Validation Accuracy', model.score(X_val_scaled, y_val))\n",
        "\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "X_test_imputed = imputer.transform(X_test_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "y_pred = model.predict(X_test_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X68B66nAbIHS",
        "colab_type": "text"
      },
      "source": [
        "#### Pipelines can help you write more concise code with fewer errors!\n",
        "\n",
        "Let's rewrite the code cell above, but with a pipeline.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntflqYdszSa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7EQOYZtqr00F"
      },
      "source": [
        "#### Get and plot coefficients\n",
        "\n",
        "This is slightly harder when using pipelines.\n",
        "\n",
        "The pipeline doesn't have a `.coef_` attribute. But the model inside the pipeline does. \n",
        "\n",
        "So, here's [how to access steps inside a pipeline](https://scikit-learn.org/stable/modules/compose.html#accessing-steps):\n",
        "\n",
        "> Pipeline’s `named_steps` attribute allows accessing steps by name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1ky4J1ru9Jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline.named_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RFlfjFN1gyjI",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = pipeline.named_steps['logisticregression']\n",
        "encoder = pipeline.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val).columns\n",
        "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
        "plt.figure(figsize=(10,30))\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_J_Imz96wPl",
        "colab_type": "text"
      },
      "source": [
        "# Use scikit-learn for decision trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86L3lownYE-e",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "\n",
        "We will start with default parameters, including:\n",
        "- `max_depth=None`\n",
        "- `min_samples_leaf=1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-1_grdR-AVyJ",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5tupFXLjj6B",
        "colab_type": "text"
      },
      "source": [
        "### Plot the decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA1SmBwi6H1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot tree\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "model = pipeline.named_steps['decisiontreeclassifier']\n",
        "encoder = pipeline.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val).columns\n",
        "\n",
        "dot_data = export_graphviz(model, \n",
        "                           out_file=None, \n",
        "                           max_depth=3, \n",
        "                           feature_names=encoded_columns,\n",
        "                           class_names=model.classes_, \n",
        "                           impurity=False, \n",
        "                           filled=True, \n",
        "                           proportion=True, \n",
        "                           rounded=True)   \n",
        "display(graphviz.Source(dot_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZYdLLQoMcXp",
        "colab_type": "text"
      },
      "source": [
        "### Reduce complexity of the decision tree\n",
        "\n",
        "We can use the `min_samples_leaf` parameter to reduce model complexity.\n",
        "\n",
        "It's explained in [scikit-learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
        "\n",
        "Also in [A Visual Introduction to Machine Learning, Part 2](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/):\n",
        "\n",
        "> Models can be adjusted to change the way they fit the data. These 'settings' are called [hyper]parameters. An example of a decision-tree [hyper]parameter is the _minimum node size_, which regulates the creation of new splits. A node will not split if the number of data points it contains is below the minimum node size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGv9jYsmgSr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cUW3dabWAWPk"
      },
      "source": [
        "# Get and interpret feature importances of a tree-based model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dSAxgO9sLsx",
        "colab_type": "text"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVJcNKY_sso-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KkV4TjcNAPzp"
      },
      "source": [
        "# Understand why decision trees are useful to model non-linear, non-monotonic relationships and feature interactions\n",
        "\n",
        "#### What does _(non)monotonic_ mean?!?!\n",
        "- See Figures 1-3 in Wikipedia's article, [Monotonic function](https://en.wikipedia.org/wiki/Monotonic_function)\n",
        "- See [World Population Growth, 1700-2010](https://ourworldindata.org/world-population-growth-past-future). World Population is non-linear and monotonic. Annual growth rate is non-linear and non-monotonic.\n",
        "- See [Accidents per Mile Driven, by Driver Age](http://howwedrive.com/2009/02/20/whats-the-real-risk-of-older-drivers/). This is non-linear and non-monotonic.\n",
        "\n",
        "#### What does _feature interactions_ mean?!?!\n",
        "- See the explanation in [_Interpretable Machine Learning_, Chapter 5.4.1, Feature Interaction](https://christophm.github.io/interpretable-ml-book/interaction.html#feature-interaction).\n",
        "- See the exploration in this notebook, under the heading ***Example #3: Simple housing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMW9JbD6wZ28"
      },
      "source": [
        "## Example 1: Predict which waterpumps are functional, just based on location\n",
        "(2 features, non-linear, feature interactions, classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0s7t37euwd41"
      },
      "source": [
        "### Compare a Logistic Regression with 2 features, longitude & latitude ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44TUfQpeo64d",
        "colab": {}
      },
      "source": [
        "train_location = X_train[['longitude', 'latitude']].copy()\n",
        "val_location = X_val[['longitude', 'latitude']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X_xIa_1Kozfl",
        "colab": {}
      },
      "source": [
        "# With just long & lat, a Logistic Regression can't beat the majority classifier baseline\n",
        "\n",
        "lr = make_pipeline(\n",
        "    SimpleImputer(),\n",
        "    LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
        ")\n",
        "\n",
        "lr.fit(train_location, y_train)\n",
        "print('Logistic Regression:')\n",
        "print('Train Accuracy', lr.score(train_location, y_train))\n",
        "print('Validation Accuracy', lr.score(val_location, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yD31msBkwlVQ"
      },
      "source": [
        "### ... versus a Decision Tree Classifier with 2 features, longitude & latitude\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "\n",
        "We will start with default parameters, including:\n",
        "- `max_depth=None`\n",
        "- `min_samples_leaf=1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RbY4s-vjiMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# With the same 2 features, the Decision Tree's validation accuracy is \n",
        "# almost 10 percentage points better\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = make_pipeline(\n",
        "    SimpleImputer(), \n",
        "    DecisionTreeClassifier(max_depth=16, random_state=42)\n",
        ")\n",
        "\n",
        "dt.fit(train_location, y_train)\n",
        "print('Logistic Regression:')\n",
        "print('Train Accuracy', dt.score(train_location, y_train))\n",
        "print('Validation Accuracy', dt.score(val_location, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFvsu2onNXw7",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the logistic regression predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yeINd4f8bRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_heatmap(lr, train_location, features=['longitude', 'latitude'], \n",
        "             class_index=0, title='Logstic Regression, predicted probability, \"functional\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebepdSHp4X4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.Series(lr.named_steps['logisticregression'].coef_[0], \n",
        "          train_location.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJZSMvW7Nd97",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the decision tree predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ-QtMDK-uX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_heatmap(dt, train_location, features=['longitude', 'latitude'], \n",
        "             class_index=0, title='Logstic Regression, predicted probability, \"functional\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdjg7U_jNlJG",
        "colab_type": "text"
      },
      "source": [
        "### How does a tree grow? Branch by branch!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Jhv0_VId9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "for max_depth in [1,2,3]:\n",
        "    \n",
        "    # Fit decision tree\n",
        "    dt = make_pipeline(\n",
        "        SimpleImputer(), \n",
        "        DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
        "    )\n",
        "    dt.fit(train_location, y_train)\n",
        "    \n",
        "    # Display depth & scores\n",
        "    display(HTML(f'Max Depth {max_depth}'))\n",
        "    display(HTML(f'Train Accuracy {dt.score(train_location, y_train):.2f}'))\n",
        "    display(HTML(f'Validation Accuracy {dt.score(val_location, y_val):.2f}'))\n",
        "    \n",
        "    # Plot heatmap of predicted probabilities\n",
        "    pred_heatmap(dt, train_location, features=['longitude', 'latitude'], \n",
        "                 class_index=0, title='Predicted probability, \"functional\"')\n",
        "    \n",
        "    # Plot tree\n",
        "    # https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
        "    dot_data = export_graphviz(dt.named_steps['decisiontreeclassifier'], \n",
        "                               out_file=None, \n",
        "                               max_depth=3, \n",
        "                               feature_names=train_location.columns,\n",
        "                               class_names=dt.classes_, \n",
        "                               impurity=False, \n",
        "                               filled=True, \n",
        "                               proportion=True, \n",
        "                               rounded=True)   \n",
        "    display(graphviz.Source(dot_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm9ta54J66CV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for max_depth in range(4,21):\n",
        "    \n",
        "    # Fit decision tree\n",
        "    dt = make_pipeline(\n",
        "        SimpleImputer(), \n",
        "        DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
        "    )\n",
        "    dt.fit(train_location, y_train)\n",
        "    \n",
        "    # Display depth & scores\n",
        "    display(HTML(f'Max Depth {max_depth}'))\n",
        "    display(HTML(f'Train Accuracy {dt.score(train_location, y_train):.2f}'))\n",
        "    display(HTML(f'Validation Accuracy {dt.score(val_location, y_val):.2f}'))\n",
        "    \n",
        "    # Plot heatmap of predicted probabilities\n",
        "    pred_heatmap(dt, train_location, features=['longitude', 'latitude'], \n",
        "                 class_index=0, title='Predicted probability, \"functional\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5zlKegIZBbGr"
      },
      "source": [
        "## Example 2: predicting golf putts\n",
        "(1 feature, non-linear, regression)\n",
        "\n",
        "https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GjvPHk-FBayo",
        "colab": {}
      },
      "source": [
        "columns = ['distance', 'tries', 'successes']\n",
        "data = [[2, 1443, 1346],\n",
        "        [3, 694, 577],\n",
        "        [4, 455, 337],\n",
        "        [5, 353, 208],\n",
        "        [6, 272, 149],\n",
        "        [7, 256, 136],\n",
        "        [8, 240, 111],\n",
        "        [9, 217, 69],\n",
        "        [10, 200, 67],\n",
        "        [11, 237, 75],\n",
        "        [12, 202, 52],\n",
        "        [13, 192, 46],\n",
        "        [14, 174, 54],\n",
        "        [15, 167, 28],\n",
        "        [16, 201, 27],\n",
        "        [17, 195, 31],\n",
        "        [18, 191, 33],\n",
        "        [19, 147, 20],\n",
        "        [20, 152, 24]]\n",
        "\n",
        "putts = pd.DataFrame(columns=columns, data=data)\n",
        "putts['rate of success'] = putts['successes'] / putts['tries']\n",
        "putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tr-83jOEBsxK"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x988kCamBpbn",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "putts_X = putts[['distance']]\n",
        "putts_y = putts['rate of success']\n",
        "lr = LinearRegression()\n",
        "lr.fit(putts_X, putts_y)\n",
        "print('R^2 Score', lr.score(putts_X, putts_y))\n",
        "ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
        "ax.plot(putts_X, lr.predict(putts_X));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MjLGrZjfFGD-"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EpFIetrsB2yc",
        "colab": {}
      },
      "source": [
        "import graphviz\n",
        "from ipywidgets import interact\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
        "\n",
        "def viztree(decision_tree, feature_names):\n",
        "    dot_data = export_graphviz(decision_tree, out_file=None, feature_names=feature_names, \n",
        "                               filled=True, rounded=True)   \n",
        "    return graphviz.Source(dot_data)\n",
        "\n",
        "def putts_tree(max_depth=1):\n",
        "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
        "    tree.fit(putts_X, putts_y)\n",
        "    print('R^2 Score', tree.score(putts_X, putts_y))\n",
        "    ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
        "    ax.step(putts_X, tree.predict(putts_X), where='mid')\n",
        "    plt.show()\n",
        "    display(viztree(tree, feature_names=['distance']))\n",
        "\n",
        "interact(putts_tree, max_depth=(1,6,1));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WBQJB0PVFtIE"
      },
      "source": [
        "## Example 3a: Simple housing \n",
        "(2 features, regression)\n",
        "\n",
        "https://christophm.github.io/interpretable-ml-book/interaction.html#feature-interaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SHQ6oh1uFzHO",
        "colab": {}
      },
      "source": [
        "columns = ['Price', 'Good Location', 'Big Size']\n",
        "\n",
        "data = [[300000, 1, 1], \n",
        "        [200000, 1, 0], \n",
        "        [250000, 0, 1], \n",
        "        [150000, 0, 0]]\n",
        "\n",
        "house = pd.DataFrame(columns=columns, data=data)\n",
        "house"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cH8IVe6GGAsm"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9uMiKdzF6Ud",
        "colab": {}
      },
      "source": [
        "house_X = house.drop(columns='Price')\n",
        "house_y = house['Price']\n",
        "lr = LinearRegression()\n",
        "lr.fit(house_X, house_y)\n",
        "print('R^2', lr.score(house_X, house_y))\n",
        "print('Intercept \\t', lr.intercept_)\n",
        "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
        "print(coefficients.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q7AASlWVGMD4"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sWlqO_usGKS8",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "tree.fit(house_X, house_y)\n",
        "print('R^2', tree.score(house_X, house_y))\n",
        "viztree(tree, feature_names=house_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MZkaOREUGzYp"
      },
      "source": [
        "## Example 3b: Simple housing, with a twist: _Feature Interaction_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6HE0_pibGzJj",
        "colab": {}
      },
      "source": [
        "house.loc[0, 'Price'] = 400000\n",
        "house_X = house.drop(columns='Price')\n",
        "house_y = house['Price']\n",
        "house"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rh7ZJe7GHCSp"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YYINxJkdG_Q2",
        "colab": {}
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(house_X, house_y)\n",
        "print('R^2', lr.score(house_X, house_y))\n",
        "print('Intercept \\t', lr.intercept_)\n",
        "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
        "print(coefficients.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9KIVsFO_HSmS"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r6JYZBZBHIX2",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "tree.fit(house_X, house_y)\n",
        "print('R^2', tree.score(house_X, house_y))\n",
        "viztree(tree, feature_names=house_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}